{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkkKcya4xYy2",
        "outputId": "e95a6d84-df64-41b6-9c5f-b44b29632436"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from tflearn) (8.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCzAEBfhxAHe",
        "outputId": "93a8051e-eb2c-4db1-ceab-9fdc097688d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Importing Libraries required for NLP\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Importing Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf   #version 1.13.2\n",
        "from tensorflow.python.framework import ops\n",
        "import numpy as np\n",
        "import tflearn            #version 0.3.2\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rKryUkFWxAHf"
      },
      "outputs": [],
      "source": [
        "# importing our intent file used for training the model.\n",
        "with open(\"intents.json\") as json_data: \n",
        "    intents = json.load(json_data)      # Loading data from intents.json file to var intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6L2shq-NxAHg"
      },
      "outputs": [],
      "source": [
        "# Empty lists for appending the data after processing NLP\n",
        "words=[]\n",
        "documents = []\n",
        "classes = []\n",
        "\n",
        "\n",
        "# This list will be used for ignoring all unwanted punctuation marks.\n",
        "ignore = [\"?\"]\n",
        "\n",
        "# Starting a loop through each intent in intents[\"patterns\"]\n",
        "for intent in intents[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        \n",
        "        # tokenizing each and every word in the sentence by using word tokenizer and storing in w\n",
        "        w = nltk.word_tokenize(pattern) \n",
        "        #print(w)\n",
        "        \n",
        "        # Adding tokenized words to words empty list that we created\n",
        "        words.extend(w) \n",
        "        #print(words)\n",
        "        \n",
        "        # Adding words to documents with tag given in intents file\n",
        "        documents.append((w, intent[\"tag\"]))\n",
        "        #print(documents)\n",
        "        \n",
        "        # Adding only tag to our classes list\n",
        "        if intent[\"tag\"] not in classes:      \n",
        "            classes.append(intent[\"tag\"])  #If tag is not present in classes[] then it will append into it.\n",
        "            #print(classes)\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNuf2CGvxAHh",
        "outputId": "9a784975-6a67-414a-915f-3c0394101cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33 Documents \n",
            "\n",
            "11 Classes \n",
            "\n",
            "68 Stemmed Words \n"
          ]
        }
      ],
      "source": [
        "#Performing Stemming by using stemmer.stem() nd lower each word \n",
        "#Running loop in words[] and ignoring punctuation marks present in ignore[]\n",
        "\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]  \n",
        "words = sorted(list(set(words)))  #Removing Duplicates in words[]\n",
        "\n",
        "#Removing Duplicate Classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "#Printing length of lists we formed\n",
        "print(len(documents),\"Documents \\n\")\n",
        "print(len(classes),\"Classes \\n\")\n",
        "print(len(words), \"Stemmed Words \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wR2Xi1-VxAHj"
      },
      "outputs": [],
      "source": [
        "#Creating Training Data which will be furthur used for training\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "#Creating empty array for output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "#Creating Training set and bag of words for each sentence\n",
        "for doc in documents:\n",
        "    bag = [] #Initialising empty bag of words\n",
        "\n",
        "    pattern_words = doc[0] #Storing list of tokenized words for the documents[] tp pattern_words\n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Again Stemming each word from pattern_words\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
        "    #print(pattern_words)\n",
        "    \n",
        "    #Creating bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "        \n",
        "    #It will give output 1 for curent tag and 0 for all other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] =1 \n",
        "    training.append([bag, output_row])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rEkTDr6xAHk",
        "outputId": "654e6940-84e4-4068-bec1-dbd057dae788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-46948735235d>:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training = np.array(training) #Converting training data into numpy array\n"
          ]
        }
      ],
      "source": [
        "random.shuffle(training) #Suffling the data or features\n",
        "training = np.array(training) #Converting training data into numpy array\n",
        "\n",
        "#Creating Training Lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe6HoU_2xAHl",
        "outputId": "3e5ee614-a68d-458b-af30-7e06c41fde99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 4999  | total loss: \u001b[1m\u001b[32m0.29142\u001b[0m\u001b[0m | time: 0.017s\n",
            "| Adam | epoch: 1000 | loss: 0.29142 - acc: 0.9713 -- iter: 32/33\n",
            "Training Step: 5000  | total loss: \u001b[1m\u001b[32m0.26519\u001b[0m\u001b[0m | time: 0.020s\n",
            "| Adam | epoch: 1000 | loss: 0.26519 - acc: 0.9742 -- iter: 33/33\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "ops.reset_default_graph() #Reset Underlying Graph data\n",
        "\n",
        "#Building our own Neural Network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "#Defining Model and setting up tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir=\"tflearn_logs\") \n",
        "\n",
        "#Now we have setup model, now we need to train that model by fitting data into it by model.fit()\n",
        "#n_epoch is the number of times that model will se our data during training\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True) \n",
        "model.save(\"model.tflearn\") #Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "G1flWgnxxAHm"
      },
      "outputs": [],
      "source": [
        "#Importing pickle module\n",
        "import pickle\n",
        "\n",
        "#Dumping training data by using dump() and writing it into training_data in binary mode\n",
        "pickle.dump({\"words\":words, \"classes\":classes, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7WBZlUPixAHn"
      },
      "outputs": [],
      "source": [
        "#Restoring all data structure\n",
        "data = pickle.load(open(\"training_data\",\"rb\"))\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "WvHRxd6LxAHn"
      },
      "outputs": [],
      "source": [
        "with open(\"intents.json\") as json_data:\n",
        "    intents = json.load(json_data)  #Loading our json_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "qbxbzvKHxAHo"
      },
      "outputs": [],
      "source": [
        "# Loading the saved model\n",
        "model.load(\"./model.tflearn\") #Loading training model which we saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "KkrNzBLHxAHo"
      },
      "outputs": [],
      "source": [
        "#Cleaning User Input\n",
        "def clean_up_sentence(sentence):\n",
        "    \n",
        "    # Tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence) #Again tokenizing the sentence\n",
        "    \n",
        "    #Stemming each word from the user's input\n",
        "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "\n",
        "    return sentence_words\n",
        "\n",
        "#Returning bag of words array: 0 or 1 or each word in the bag that exists in as we have declared in above lines\n",
        "def bow(sentence, words, show_details=False):\n",
        "    \n",
        "    #Tokenizing the user input\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    \n",
        "    #Generating bag of words from the sentence that user entered\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(\"Found in bag: %s\"% w)\n",
        "    return(np.array(bag))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "8FdCJKFxxAHp"
      },
      "outputs": [],
      "source": [
        "#Adding some context to the conversation for better results.\n",
        "\n",
        "context = {} #Create a dictionary to hold user's context\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    \n",
        "    #Generating probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    \n",
        "    #Filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    \n",
        "    #Sorting by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    \n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    \n",
        "    #If we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        \n",
        "        #Loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                \n",
        "                #Find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    \n",
        "                    #Set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        \n",
        "                        #A random response from the intent\n",
        "                        return print(\"Bot: \", random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Td-dMq2exAHq",
        "outputId": "15eb8a80-009f-4fba-d83d-678520d19922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Boot up complete...\n",
            "User: hello\n",
            "Bot:  Hi! How can I help you stay safe online?\n",
            "User: What is a Malware?\n",
            "Bot:  Malware refers to malicious software designed to harm or exploit computers, networks, and devices. Common types of malware include viruses, worms, Trojans, ransomware, and spyware. To protect against malware, keep your operating system and software up to date, use a reliable antivirus program, avoid downloading files or clicking on suspicious links, and be cautious of email attachments from unknown sources.\n",
            "User: What is Social Engineering?\n",
            "Bot:  Social engineering is the practice of manipulating people into divulging sensitive information or performing actions that may compromise their security. Attackers use psychological techniques to deceive individuals and gain unauthorized access to systems or data. Examples of social engineering attacks include phishing, pretexting, baiting, tailgating, and quid pro quo. To avoid social engineering, be cautious of sharing personal information with unknown individuals, double-check the legitimacy of requests or messages, and educate yourself about common social engineering tactics. It's important to be skeptical and verify the authenticity of any unusual requests or offers.\n",
            "User: What are strong passwords?\n",
            "Bot:  A strong password should be at least 8 characters long and include a combination of letters (both uppercase and lowercase), numbers, and symbols. Avoid using common phrases, personal information, or easily guessable patterns in your passwords. It's also important to use a unique password for each online account.\n",
            "User: How to secure your device from hackers?\n",
            "Bot:  To secure your devices, keep your operating system, apps, and antivirus software up to date. Use strong and unique passwords or biometric authentication methods. Be cautious when installing third-party apps and only download from trusted sources. Regularly back up your data, enable device tracking or remote wiping features, and avoid connecting to unknown or unsecured Wi-Fi networks. It's also recommended to enable device encryption and use a reliable firewall for added protection.\n",
            "User: exit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "boolean = True\n",
        "demand = \"\"\n",
        "print(\"Bot: Boot up complete...\")\n",
        "\n",
        "while boolean:\n",
        "    demand = str(input(\"User: \"))\n",
        "    if demand in [\"exit\", \"quit\", \"leave\", \"EXIT\", \"QUIT\", \"LEAVE\", \"Exit\", \"Quit\", \"Leave\"]:\n",
        "      boolean = False\n",
        "      break\n",
        "    response(demand)\n",
        "print(\"Bot: Goodbye!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}